{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j_parallel_spark_loader import monopartite\n",
    "\n",
    "from benchmarking.utils.spark import create_spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: neo4j.url\n",
      "Warning: Ignoring non-Spark config property: url\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.password\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.type\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.username\n",
      "Ivy Default Cache set to: /Users/alexandergilmore/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/alexandergilmore/.ivy2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-616d1d0b-5d0d-41df-b22e-655bec560348;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/alexandergilmore/Documents/projects/neo4j-parallel-spark-loader/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12;5.1.0_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12_common;5.1.0 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver;4.4.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in local-m2-cache\n",
      "\tfound org.apache.xbean#xbean-asm6-shaded;4.10 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.9.0 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in local-m2-cache\n",
      ":: resolution report :: resolve 144ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.xbean#xbean-asm6-shaded;4.10 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from local-m2-cache in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12;5.1.0_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12_common;5.1.0 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.9.0 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver;4.4.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-616d1d0b-5d0d-41df-b22e-655bec560348\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/3ms)\n",
      "25/01/07 13:47:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark_session = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/monopartite_data.csv\")\n",
    "df = spark_session.read.option(\"header\", True).csv(\"data/monopartite_data.csv\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "group_and_batch_df = monopartite.group_and_batch_spark_dataframe(df, \"source\", \"target\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-----+\n",
      "|source|target|  group|batch|\n",
      "+------+------+-------+-----+\n",
      "| 81701| 97415|1 --> 0|    1|\n",
      "| 88400| 97394|1 --> 0|    1|\n",
      "| 84293| 28891|1 --> 0|    1|\n",
      "| 70745| 89229|1 --> 0|    1|\n",
      "| 84916| 52066|1 --> 0|    1|\n",
      "| 55952| 81848|1 --> 0|    1|\n",
      "| 52618| 73183|1 --> 0|    1|\n",
      "| 71163| 39193|1 --> 0|    1|\n",
      "| 70637| 93852|1 --> 1|    0|\n",
      "| 12626| 65960|1 --> 1|    0|\n",
      "| 76256| 61327|0 --> 1|    1|\n",
      "| 54721| 72127|0 --> 1|    1|\n",
      "| 18296| 98682|0 --> 1|    1|\n",
      "| 56918| 64645|0 --> 1|    1|\n",
      "| 91904| 10715|0 --> 1|    1|\n",
      "| 54269| 72396|0 --> 1|    1|\n",
      "| 65202| 13815|0 --> 0|    0|\n",
      "| 93007| 95821|0 --> 0|    0|\n",
      "| 80390| 77535|0 --> 0|    0|\n",
      "| 84911| 72255|0 --> 0|    0|\n",
      "+------+------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_and_batch_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = group_and_batch_df.select(\"target\", \"group\", \"batch\").drop_duplicates().groupBy(\"target\", \"batch\").count().orderBy(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|target|batch|count|\n",
      "+------+-----+-----+\n",
      "|  8270|    0|    1|\n",
      "| 75630|    1|    1|\n",
      "| 82293|    1|    1|\n",
      "| 53468|    1|    1|\n",
      "| 44297|    0|    1|\n",
      "| 59665|    1|    1|\n",
      "| 41738|    1|    1|\n",
      "| 22706|    1|    1|\n",
      "| 79800|    0|    1|\n",
      "| 28480|    0|    1|\n",
      "| 76211|    1|    1|\n",
      "| 96548|    0|    1|\n",
      "| 49266|    0|    1|\n",
      "| 83080|    1|    1|\n",
      "| 20550|    0|    1|\n",
      "| 53033|    0|    1|\n",
      "| 59770|    1|    1|\n",
      "| 59774|    0|    1|\n",
      "| 52790|    0|    1|\n",
      "| 75544|    0|    1|\n",
      "+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = group_and_batch_df.select(\"source\", \"group\", \"batch\").withColumnRenamed(\"source\", \"source_or_target\").union(group_and_batch_df.select(\"target\", \"group\", \"batch\").withColumnRenamed(\"target\", \"source_or_target\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_grouped = stack.select(\"source_or_target\", \"group\", \"batch\").drop_duplicates().groupBy(\"source_or_target\", \"batch\").count().orderBy(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-----+\n",
      "|source_or_target|batch|count|\n",
      "+----------------+-----+-----+\n",
      "|           22811|    1|    2|\n",
      "|           47772|    1|    2|\n",
      "|           72959|    1|    2|\n",
      "|           98492|    1|    2|\n",
      "|           55936|    1|    2|\n",
      "|           56770|    1|    2|\n",
      "|           98773|    1|    2|\n",
      "|           50573|    1|    2|\n",
      "|           83723|    1|    2|\n",
      "|           38282|    1|    2|\n",
      "|           25763|    1|    2|\n",
      "|           99926|    1|    2|\n",
      "|           88881|    1|    2|\n",
      "|           89668|    1|    2|\n",
      "|           95252|    1|    2|\n",
      "|           36928|    1|    2|\n",
      "|           52622|    1|    2|\n",
      "|           44548|    1|    2|\n",
      "|           18474|    1|    2|\n",
      "|           55848|    1|    2|\n",
      "+----------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stack_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-----+\n",
      "|source|target|  group|batch|\n",
      "+------+------+-------+-----+\n",
      "| 22811| 61140|1 --> 0|    1|\n",
      "| 97378| 22811|0 --> 1|    1|\n",
      "+------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_and_batch_df.select(\"*\").where(\"source = 22811 or target = 22811\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
